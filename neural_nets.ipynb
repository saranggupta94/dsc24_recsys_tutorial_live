{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef21bde-0104-4cc6-a1b6-b5ba8121dd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/venv1/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51a14a10-7314-48a5-b1d7-f8bbad03634d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1b937c2-d155-443c-b6df-fd6801ba791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_titles(titles, model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Encode movie titles using sentence transformer\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(titles, convert_to_tensor=True)\n",
    "    return embeddings, model.get_sentence_embedding_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfc66089-9c27-47e7-a3cc-1eb10d4cec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_tensors(user_data, movie_data, ratings, title_embeddings):\n",
    "    \"\"\"\n",
    "    Create tensors for the dataset\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'user_ids': torch.tensor(user_data['user_id'].values, dtype=torch.long),\n",
    "        'gender': torch.tensor(user_data['gender_encoded'].values, dtype=torch.long),\n",
    "        'age': torch.tensor(user_data['age'].values, dtype=torch.float),\n",
    "        'occupation': torch.tensor(user_data['occupation'].values, dtype=torch.long),\n",
    "        'movie_ids': torch.tensor(movie_data['movie_id'].values, dtype=torch.long),\n",
    "        'genres': torch.tensor(np.stack(movie_data['genres_encoded']), dtype=torch.float),\n",
    "        'title_embeddings': title_embeddings,\n",
    "        'ratings': torch.tensor(ratings, dtype=torch.float)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "029a2a07-be5c-4dd7-a526-491fea7a9028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading ratings file\n",
    "ratings = pd.read_csv('data-1m/ratings.csv', \n",
    "                    sep='\\t', #Note that the separator here is \"\\t\"\n",
    "                    encoding='latin-1',\n",
    "                    engine='python',\n",
    "                    index_col=0\n",
    "                     ) \n",
    "\n",
    "# Reading users file\n",
    "users = pd.read_csv('data-1m/users.csv', \n",
    "                    sep='\\t', #Note that the separator here is \"\\t\"\n",
    "                    encoding='latin-1',\n",
    "                    engine='python',\n",
    "                    index_col=0\n",
    "                     )\n",
    "\n",
    "# # Reading movies file\n",
    "movies = pd.read_csv('data-1m/movies.csv', \n",
    "                    sep='\\t', #Note that the separator here is \"\\t\"\n",
    "                    encoding='latin-1',\n",
    "                    engine='python',\n",
    "                    index_col=0\n",
    "                     ) \n",
    "\n",
    "ratings = ratings.drop(columns=['user_emb_id', 'movie_emb_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f7702e2-4625-4d74-95ca-07a6a1203592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie ID range: 0 to 3882\n",
      "User ID range: 0 to 6039\n"
     ]
    }
   ],
   "source": [
    "# Create mappings for movie and user IDs\n",
    "movie_id_map = {old_id: new_id for new_id, old_id in enumerate(sorted(movies['movie_id'].unique()))}\n",
    "user_id_map = {old_id: new_id for new_id, old_id in enumerate(sorted(users['user_id'].unique()))}\n",
    "\n",
    "# Save the mappings for later use (optional)\n",
    "movie_id_reverse_map = {v: k for k, v in movie_id_map.items()}\n",
    "user_id_reverse_map = {v: k for k, v in user_id_map.items()}\n",
    "\n",
    "# Apply the mappings\n",
    "movies['movie_id'] = movies['movie_id'].map(movie_id_map)\n",
    "users['user_id'] = users['user_id'].map(user_id_map)\n",
    "ratings['movie_id'] = ratings['movie_id'].map(movie_id_map)\n",
    "ratings['user_id'] = ratings['user_id'].map(user_id_map)\n",
    "\n",
    "# Verify the mapping worked correctly\n",
    "print(\"Movie ID range:\", movies['movie_id'].min(), \"to\", movies['movie_id'].max())\n",
    "print(\"User ID range:\", users['user_id'].min(), \"to\", users['user_id'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "850f67b2-92df-4f59-bb3d-180eb76c5c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3883, 384])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b42f7f63-35ff-4595-ba55-fc5c14beae90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0828,  0.0530,  0.0536,  ...,  0.0226,  0.0538,  0.1030],\n",
       "        [-0.1053,  0.1508, -0.0264,  ...,  0.0106, -0.0726,  0.0086],\n",
       "        [-0.0988,  0.0177, -0.0527,  ..., -0.0120,  0.0303,  0.0004],\n",
       "        ...,\n",
       "        [-0.0730, -0.0164, -0.0606,  ..., -0.0033, -0.0181,  0.0027],\n",
       "        [-0.0611, -0.0336,  0.0070,  ...,  0.0937, -0.0130,  0.0063],\n",
       "        [-0.1084,  0.0172, -0.0453,  ...,  0.0170,  0.0158,  0.0596]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7de96f05-6db5-4b4a-8268-1a219c105e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_mask = torch.isnan(title_embeddings).any(dim=1)\n",
    "nan_indices = torch.where(nan_mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e7b25ab-b742-44db-83ed-6250fe5b11bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], device='mps:0', dtype=torch.int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a3baa46-0fa8-40f3-9ed0-e2171a2c0a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings with NaN values: 0\n",
      "No NaN values found after moving tensor to CPU\n",
      "\n",
      "Double checking with numpy:\n",
      "Number of embeddings with NaN values (numpy check): 0\n"
     ]
    }
   ],
   "source": [
    "# First move the tensor to CPU\n",
    "title_embeddings_cpu = title_embeddings.to('cpu')\n",
    "\n",
    "# Now check for NaNs\n",
    "nan_mask = torch.isnan(title_embeddings_cpu).any(dim=1)\n",
    "nan_indices = torch.where(nan_mask)[0]\n",
    "\n",
    "print(f\"Number of embeddings with NaN values: {len(nan_indices)}\")\n",
    "\n",
    "if len(nan_indices) > 0:\n",
    "    print(\"\\nTitles with NaN embeddings:\")\n",
    "    for idx in nan_indices:\n",
    "        print(f\"Index {idx}: {movies.iloc[idx]['title']}\")\n",
    "    \n",
    "    # Get detailed statistics about NaN values\n",
    "    nan_per_dimension = torch.isnan(title_embeddings_cpu).sum(dim=0)\n",
    "    print(f\"\\nDimensions with NaNs: {torch.sum(nan_per_dimension > 0).item()}\")\n",
    "    \n",
    "    # Show a sample of problematic embeddings\n",
    "    print(\"\\nSample of first problematic embedding:\")\n",
    "    print(title_embeddings_cpu[nan_indices[0]])\n",
    "else:\n",
    "    print(\"No NaN values found after moving tensor to CPU\")\n",
    "    \n",
    "    # Let's double check with a different method\n",
    "    print(\"\\nDouble checking with numpy:\")\n",
    "    title_embeddings_np = title_embeddings_cpu.numpy()\n",
    "    nan_mask_np = np.isnan(title_embeddings_np).any(axis=1)\n",
    "    nan_indices_np = np.where(nan_mask_np)[0]\n",
    "    print(f\"Number of embeddings with NaN values (numpy check): {len(nan_indices_np)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf9a175c-696c-43bf-8a24-6a7ba65a96a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensor statistics:\n",
      "Min value: -0.24947617948055267\n",
      "Max value: 0.24644768238067627\n",
      "Mean value: 0.0005378836067393422\n",
      "Shape: torch.Size([3883, 384])\n",
      "\n",
      "Number of embeddings with infinite values: 0\n",
      "Number of embeddings with values >100: 0\n"
     ]
    }
   ],
   "source": [
    "# Additional checks\n",
    "print(\"\\nTensor statistics:\")\n",
    "print(f\"Min value: {title_embeddings_cpu.min().item()}\")\n",
    "print(f\"Max value: {title_embeddings_cpu.max().item()}\")\n",
    "print(f\"Mean value: {title_embeddings_cpu.mean().item()}\")\n",
    "print(f\"Shape: {title_embeddings_cpu.shape}\")\n",
    "\n",
    "# Check for any infinite values\n",
    "inf_mask = torch.isinf(title_embeddings_cpu).any(dim=1)\n",
    "inf_indices = torch.where(inf_mask)[0]\n",
    "print(f\"\\nNumber of embeddings with infinite values: {len(inf_indices)}\")\n",
    "\n",
    "# Check for unusually large values\n",
    "large_mask = (title_embeddings_cpu.abs() > 100).any(dim=1)\n",
    "large_indices = torch.where(large_mask)[0]\n",
    "print(f\"Number of embeddings with values >100: {len(large_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "373381bf-6648-47e2-b7d8-cefb0598bdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  ..., False, False, False], device='mps:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03ffb433-e414-4e52-b45e-c15023230f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode titles\n",
    "title_embeddings, title_embedding_dim = encode_titles(movies['title'].values)\n",
    "movies['genres'] = movies['genres'].str.split('|')\n",
    "movies['genres'] = movies['genres'].fillna(\"\").astype('str') #Convert genres to string values\n",
    "\n",
    "# Process genres\n",
    "mlb = MultiLabelBinarizer()\n",
    "movies['genres'] = movies['genres'].apply(eval) #List is a string, convert that into actual list\n",
    "genres_encoded = mlb.fit_transform(movies['genres'])\n",
    "movies['genres_encoded'] = list(genres_encoded)\n",
    "\n",
    "# Encode categorical variables\n",
    "gender_encoder = LabelEncoder()\n",
    "users['gender_encoded'] = gender_encoder.fit_transform(users['gender'])\n",
    "\n",
    "# Normalize age\n",
    "users['age'] = users['age'].astype(float)\n",
    "\n",
    "# Merge data\n",
    "data = ratings.merge(users, on='user_id').merge(movies, on='movie_id')\n",
    "\n",
    "# Split data\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create tensors for train and validation sets\n",
    "train_tensors = create_dataset_tensors(\n",
    "    train_data[['user_id', 'gender_encoded', 'age', 'occupation']],\n",
    "    train_data[['movie_id', 'genres_encoded']],\n",
    "    train_data['rating'].values / 5.0,\n",
    "    title_embeddings[train_data.index]\n",
    ")\n",
    "\n",
    "val_tensors = create_dataset_tensors(\n",
    "    val_data[['user_id', 'gender_encoded', 'age', 'occupation']],\n",
    "    val_data[['movie_id', 'genres_encoded']],\n",
    "    val_data['rating'].values / 5.0,\n",
    "    title_embeddings[val_data.index]\n",
    ")\n",
    "\n",
    "model_dims = {\n",
    "    'n_users': len(users['user_id'].unique()),\n",
    "    'n_movies': len(movies['movie_id'].unique()),\n",
    "    'n_genres': genres_encoded.shape[1],\n",
    "    'n_genders': len(gender_encoder.classes_),\n",
    "    'n_occupations': users['occupation'].nunique(),\n",
    "    'title_embedding_dim': title_embedding_dim\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3ef1815-8cc3-4ac8-b81e-689ad0b3f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerNet(nn.Module):\n",
    "    def __init__(self, n_users, n_movies, n_genres, n_genders, n_occupations, \n",
    "                 title_embedding_dim, embedding_dim=64):\n",
    "        super(TwoTowerNet, self).__init__()\n",
    "        \n",
    "        # Print dimensions during initialization\n",
    "        print(f\"Initializing model with dimensions:\")\n",
    "        print(f\"n_users: {n_users}\")\n",
    "        print(f\"n_movies: {n_movies}\")\n",
    "        print(f\"n_genres: {n_genres}\")\n",
    "        print(f\"n_genders: {n_genders}\")\n",
    "        print(f\"n_occupations: {n_occupations}\")\n",
    "        print(f\"title_embedding_dim: {title_embedding_dim}\")\n",
    "        \n",
    "        # User embeddings\n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.gender_embedding = nn.Embedding(n_genders, embedding_dim // 4)\n",
    "        self.occupation_embedding = nn.Embedding(n_occupations, embedding_dim // 4)\n",
    "        \n",
    "        # User tower - simplified\n",
    "        self.user_tower = nn.Sequential(\n",
    "            nn.Linear(embedding_dim + embedding_dim//4 + embedding_dim//4 + 1, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Movie embeddings\n",
    "        self.movie_embedding = nn.Embedding(n_movies, embedding_dim)\n",
    "        self.title_projection = nn.Linear(title_embedding_dim, embedding_dim)\n",
    "        \n",
    "        # Movie tower - simplified\n",
    "        self.movie_tower = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 2 + n_genres, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Final prediction - simplified\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        # Debug input shapes and values\n",
    "        for key, value in batch.items():\n",
    "            if torch.is_tensor(value):\n",
    "                print(f\"\\n{key}:\")\n",
    "                print(f\"Shape: {value.shape}\")\n",
    "                print(f\"Contains NaN: {torch.isnan(value).any().item()}\")\n",
    "                print(f\"Min: {value.min().item():.4f}, Max: {value.max().item():.4f}\")\n",
    "        \n",
    "        # 1. User embeddings\n",
    "        user_emb = self.user_embedding(batch['user_ids'])\n",
    "        print(\"\\nAfter user embedding:\")\n",
    "        print(f\"Shape: {user_emb.shape}\")\n",
    "        print(f\"Contains NaN: {torch.isnan(user_emb).any().item()}\")\n",
    "        \n",
    "        gender_emb = self.gender_embedding(batch['gender'])\n",
    "        print(\"\\nAfter gender embedding:\")\n",
    "        print(f\"Shape: {gender_emb.shape}\")\n",
    "        print(f\"Contains NaN: {torch.isnan(gender_emb).any().item()}\")\n",
    "        \n",
    "        occ_emb = self.occupation_embedding(batch['occupation'])\n",
    "        print(\"\\nAfter occupation embedding:\")\n",
    "        print(f\"Shape: {occ_emb.shape}\")\n",
    "        print(f\"Contains NaN: {torch.isnan(occ_emb).any().item()}\")\n",
    "        \n",
    "        # 2. Process age\n",
    "        age = batch['age'].unsqueeze(1) / 100.0\n",
    "        print(\"\\nAfter age processing:\")\n",
    "        print(f\"Shape: {age.shape}\")\n",
    "        print(f\"Contains NaN: {torch.isnan(age).any().item()}\")\n",
    "        \n",
    "        # 3. Combine user features\n",
    "        user_features = torch.cat([user_emb, gender_emb, occ_emb, age], dim=1)\n",
    "        print(\"\\nAfter concatenating user features:\")\n",
    "        print(f\"Shape: {user_features.shape}\")\n",
    "        print(f\"Contains NaN: {torch.isnan(user_features).any().item()}\")\n",
    "        \n",
    "        # 4. User tower\n",
    "        user_vector = self.user_tower(user_features)\n",
    "        print(\"\\nAfter user tower:\")\n",
    "        print(f\"Shape: {user_vector.shape}\")\n",
    "        print(f\"Contains NaN: {torch.isnan(user_vector).any().item()}\")\n",
    "        \n",
    "        # 5. Movie embeddings\n",
    "        movie_emb = self.movie_embedding(batch['movie_ids'])\n",
    "        print(\"\\nAfter movie embedding:\")\n",
    "        print(f\"Shape: {movie_emb.shape}\")\n",
    "        print(f\"Contains NaN: {torch.isnan(movie_emb).any().item()}\")\n",
    "        \n",
    "        title_emb = self.title_projection(batch['title_embeddings'])\n",
    "        print(\"\\nAfter title projection:\")\n",
    "        print(f\"Shape: {title_emb.shape}\")\n",
    "        print(f\"Contains NaN: {torch.isnan(title_emb).any().item()}\")\n",
    "        \n",
    "        # 6. Combine movie features\n",
    "        movie_features = torch.cat([movie_emb, title_emb, batch['genres']], dim=1)\n",
    "        print(\"\\nAfter concatenating movie features:\")\n",
    "        print(f\"Shape: {movie_features.shape}\")\n",
    "        print(f\"Contains NaN: {torch.isnan(movie_features).any().item()}\")\n",
    "        \n",
    "        # 7. Movie tower\n",
    "        movie_vector = self.movie_tower(movie_features)\n",
    "        print(\"\\nAfter movie tower:\")\n",
    "        print(f\"Shape: {movie_vector.shape}\")\n",
    "        print(f\"Contains NaN: {torch.isnan(movie_vector).any().item()}\")\n",
    "        \n",
    "        # 8. Final prediction\n",
    "        combined = torch.cat([user_vector, movie_vector], dim=1)\n",
    "        print(\"\\nAfter combining vectors:\")\n",
    "        print(f\"Shape: {combined.shape}\")\n",
    "        print(f\"Contains NaN: {torch.isnan(combined).any().item()}\")\n",
    "        \n",
    "        output = self.predictor(combined)\n",
    "        print(\"\\nFinal output:\")\n",
    "        print(f\"Shape: {output.shape}\")\n",
    "        print(f\"Contains NaN: {torch.isnan(output).any().item()}\")\n",
    "        print(f\"Min: {output.min().item():.4f}, Max: {output.max().item():.4f}\")\n",
    "        \n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9004dba3-0a40-4fc5-949a-b4353ba762c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, batch, optimizer, criterion, device):\n",
    "    \"\"\"Single training step\"\"\"\n",
    "    # Move batch to device and convert to correct types\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    predictions = model(batch)\n",
    "    \n",
    "    # Print shapes and check for NaNs\n",
    "    print(f\"Predictions shape: {predictions.shape}\")\n",
    "    print(f\"Ratings shape: {batch['ratings'].shape}\")\n",
    "    print(f\"Contains NaN - Predictions: {torch.isnan(predictions).any()}, Ratings: {torch.isnan(batch['ratings']).any()}\")\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(predictions, batch['ratings'])\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d553039-a033-47a8-b2bd-4f824eb15777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model with dimensions:\n",
      "n_users: 6040\n",
      "n_movies: 3883\n",
      "n_genres: 18\n",
      "n_genders: 2\n",
      "n_occupations: 21\n",
      "title_embedding_dim: 384\n",
      "epoch: 0\n",
      "\n",
      "user_ids:\n",
      "Shape: torch.Size([800167])\n",
      "Contains NaN: False\n",
      "Min: 0.0000, Max: 6039.0000\n",
      "\n",
      "gender:\n",
      "Shape: torch.Size([800167])\n",
      "Contains NaN: False\n",
      "Min: 0.0000, Max: 1.0000\n",
      "\n",
      "age:\n",
      "Shape: torch.Size([800167])\n",
      "Contains NaN: False\n",
      "Min: 1.0000, Max: 56.0000\n",
      "\n",
      "occupation:\n",
      "Shape: torch.Size([800167])\n",
      "Contains NaN: False\n",
      "Min: 0.0000, Max: 20.0000\n",
      "\n",
      "movie_ids:\n",
      "Shape: torch.Size([800167])\n",
      "Contains NaN: False\n",
      "Min: 0.0000, Max: 3882.0000\n",
      "\n",
      "genres:\n",
      "Shape: torch.Size([800167, 18])\n",
      "Contains NaN: False\n",
      "Min: 0.0000, Max: 1.0000\n",
      "\n",
      "title_embeddings:\n",
      "Shape: torch.Size([800167, 384])\n",
      "Contains NaN: True\n",
      "Min: nan, Max: nan\n",
      "\n",
      "ratings:\n",
      "Shape: torch.Size([800167])\n",
      "Contains NaN: False\n",
      "Min: 0.2000, Max: 1.0000\n",
      "\n",
      "After user embedding:\n",
      "Shape: torch.Size([800167, 64])\n",
      "Contains NaN: False\n",
      "\n",
      "After gender embedding:\n",
      "Shape: torch.Size([800167, 16])\n",
      "Contains NaN: False\n",
      "\n",
      "After occupation embedding:\n",
      "Shape: torch.Size([800167, 16])\n",
      "Contains NaN: False\n",
      "\n",
      "After age processing:\n",
      "Shape: torch.Size([800167, 1])\n",
      "Contains NaN: False\n",
      "\n",
      "After concatenating user features:\n",
      "Shape: torch.Size([800167, 97])\n",
      "Contains NaN: False\n",
      "\n",
      "After user tower:\n",
      "Shape: torch.Size([800167, 64])\n",
      "Contains NaN: False\n",
      "\n",
      "After movie embedding:\n",
      "Shape: torch.Size([800167, 64])\n",
      "Contains NaN: False\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensor for argument weight is on cpu but expected on mps",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Forward pass on entire dataset\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(train_tensors)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, train_tensors[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mratings\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv1/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv1/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[8], line 91\u001b[0m, in \u001b[0;36mTwoTowerNet.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmovie_emb\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContains NaN: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39misnan(movie_emb)\u001b[38;5;241m.\u001b[39many()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m title_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtitle_projection(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAfter title projection:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle_emb\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv1/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv1/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/venv1/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensor for argument weight is on cpu but expected on mps"
     ]
    }
   ],
   "source": [
    "# Initialize model and move everything to CPU\n",
    "model = TwoTowerNet(**model_dims)\n",
    "model = model.to('cpu')\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    # Training\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass on entire dataset\n",
    "    predictions = model(train_tensors)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(predictions, train_tensors['ratings'])\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = model(val_tensors)\n",
    "        val_loss = criterion(val_predictions, val_tensors['ratings'])\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}:')\n",
    "    print(f'Training Loss: {loss.item():.4f}')\n",
    "    print(f'Validation Loss: {val_loss.item():.4f}')\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "        }, 'best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cde0ec-a758-4dc4-ba15-81bbdf5929e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model dimensions:\", model_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "35974cc4-dac1-4dbb-a2de-309f04373253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_tensors, criterion, device, batch_size=256):\n",
    "    \"\"\"\n",
    "    Validation step\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(val_tensors['ratings']), batch_size):\n",
    "            batch = {k: v[i:i+batch_size].to(device) for k, v in val_tensors.items()}\n",
    "            predictions = model(batch)\n",
    "            loss = criterion(predictions, batch['ratings'])\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "    \n",
    "    return total_loss / n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff959c49-8938-4a82-8db9-4636fb600e11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
